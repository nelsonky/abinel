{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:34.895065Z",
     "start_time": "2019-02-14T14:48:33.716182+01:00"
    }
   },
   "source": [
    "# More on the Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:50:24.847163Z",
     "start_time": "2019-02-14T15:50:24.841020+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:02:56.081999Z",
     "start_time": "2019-02-14T15:02:56.072548+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def plot_roc_curve(target_test, target_predicted_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, 1])\n",
    "    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:35.822397Z",
     "start_time": "2019-02-14T14:48:35.806772+01:00"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:36.266324Z",
     "start_time": "2019-02-14T14:48:36.261286+01:00"
    }
   },
   "outputs": [],
   "source": [
    "target = data['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More feature engineering and richer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to build richer models by including more features as potential predictors for our model.\n",
    "\n",
    "Categorical variables such as `data.Embarked` or `data.Sex` can be converted as boolean indicators features also known as dummy variables or one-hot-encoded features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:36.974080Z",
     "start_time": "2019-02-14T14:48:36.971155+01:00"
    }
   },
   "outputs": [],
   "source": [
    "# I want to change categorical values in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:37.463917Z",
     "start_time": "2019-02-14T14:48:37.422431+01:00"
    }
   },
   "outputs": [],
   "source": [
    "data.Sex.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:37.614911Z",
     "start_time": "2019-02-14T14:48:37.599455+01:00"
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(data.Sex, prefix='Sex').head(5)\n",
    "# dummy because it's just 0 or 1...if female it's 1\n",
    "# in terms of machine learning, of course one column is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:37.882284Z",
     "start_time": "2019-02-14T14:48:37.869944+01:00"
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(data.Embarked, prefix='Embarked').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine those new numerical features with the previous features using `pandas.concat` along `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:38.271169Z",
     "start_time": "2019-02-14T14:48:38.242818+01:00"
    }
   },
   "outputs": [],
   "source": [
    "rich_features = pd.concat([data.get(['Fare', 'Pclass', 'Age']),\n",
    "                           pd.get_dummies(data.Sex, prefix='Sex'),\n",
    "                           pd.get_dummies(data.Embarked, prefix='Embarked')],\n",
    "                          axis=1)\n",
    "# axis = 1 means concatenate columns\n",
    "rich_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction the new `Sex_male` feature is redundant with `Sex_female`. Let us drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:38.761024Z",
     "start_time": "2019-02-14T14:48:38.737003+01:00"
    }
   },
   "outputs": [],
   "source": [
    "rich_features_no_male = rich_features.drop('Sex_male', 1)\n",
    "rich_features_no_male.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us not forget to impute the median age for passengers without age information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:39.123739Z",
     "start_time": "2019-02-14T14:48:39.111730+01:00"
    }
   },
   "outputs": [],
   "source": [
    "rich_features_no_male.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:39.312094Z",
     "start_time": "2019-02-14T14:48:39.292756+01:00"
    }
   },
   "outputs": [],
   "source": [
    "rich_features_final = rich_features_no_male.fillna(rich_features_no_male.dropna().median())\n",
    "rich_features_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally cross-validate a logistic regression model on this new data an observe that the mean score has significantly increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:40.523697Z",
     "start_time": "2019-02-14T14:48:39.685061+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression(C=1.)\n",
    "scores = cross_val_score(logreg, rich_features_final, target, cv=5, scoring='accuracy')\n",
    "print(\"Logistic Regression CV scores:\")\n",
    "print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "    scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "- change the value of the parameter `C`. Does it have an impact on the score?\n",
    "\n",
    "- fit a new instance of the logistic regression model on the full dataset.\n",
    "\n",
    "- plot the weights for the features of this newly fitted logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:40.541740Z",
     "start_time": "2019-02-14T14:48:40.537058+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/04A_plot_logistic_regression_weights.py\n",
    "\n",
    "\n",
    "# Rich young women like Kate Winslet tend to survive the Titanic better\n",
    "# than poor men like Leonardo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Non-linear models: ensembles of randomized trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` also implement non linear models that are known to perform very well for data-science projects where datasets have not too many features (e.g. less than 5000).\n",
    "\n",
    "In particular let us have a look at Random Forests and Gradient Boosted Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:44.337454Z",
     "start_time": "2019-02-14T14:48:43.585002+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(rf, rich_features_final, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy')\n",
    "print(\"Random Forest CV scores:\")\n",
    "print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "    scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:45.152103Z",
     "start_time": "2019-02-14T14:48:44.708407+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                subsample=.8, max_features=.5)\n",
    "scores = cross_val_score(gb, rich_features_final, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy')\n",
    "print(\"Gradient Boosted Trees CV scores:\")\n",
    "print(\"min: {:.3f}, mean: {:.3f}, max: {:.3f}\".format(\n",
    "    scores.min(), scores.mean(), scores.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models seem to do slightly better than the logistic regression model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "- Change the value of the learning_rate and other `GradientBoostingClassifier` parameter, can you get a better mean score?\n",
    "\n",
    "- Would treating the `PClass` variable as categorical improve the models performance?\n",
    "\n",
    "- Find out which predictor variables (features) are the most informative for those models.\n",
    "\n",
    "Hints:\n",
    "\n",
    "Fitted ensembles of trees have `feature_importances_` attribute that can be used similarly to the `coef_` attribute of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:46.107830Z",
     "start_time": "2019-02-14T14:48:45.631828+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/04B_more_categorical_variables.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:48:47.831470Z",
     "start_time": "2019-02-14T14:48:47.515052+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/04C_feature_importance.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of changing the value of the learning rate manually and re-running the cross-validation, we can find the best values for the parameters automatically (assuming we are ready to wait):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:49:05.979966Z",
     "start_time": "2019-02-14T14:49:00.485815+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.grid_search.GridSearchCV import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, subsample=.8)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'max_features': [0.5, 1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "}\n",
    "gs = GridSearchCV(gb, params, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "gs.fit(rich_features_final, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sort the models by mean validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:57:21.818627Z",
     "start_time": "2019-02-14T14:57:21.807867+01:00"
    }
   },
   "outputs": [],
   "source": [
    "order=np.argsort(gs.cv_results_['mean_test_score'])[::-1]\n",
    "for i in range(gs.cv_results_['mean_test_score'].shape[0]):\n",
    "    print('mean: %f, std: %f, params: %s' % \n",
    "          (gs.cv_results_['mean_test_score'][order[i]],\n",
    "           gs.cv_results_['std_test_score'][order[i]],\n",
    "           gs.cv_results_['params'][order[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:57:35.226884Z",
     "start_time": "2019-02-14T14:57:35.221387+01:00"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:57:37.337950Z",
     "start_time": "2019-02-14T14:57:37.332464+01:00"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that the mean scores are very close to one another and almost always within one standard deviation of one another. This means that all those parameters are quite reasonable. The only parameter of importance seems to be the `learning_rate`: 0.5 seems to be a bit too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding data snooping with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing imputation in pandas, prior to computing the train test split we use data from the test to improve the accuracy of the median value that we impute on the training set. This is actually cheating. To avoid this we should compute the median of the features on the training fold and use that median value to do the imputation both on the training and validation fold for a given CV split.\n",
    "\n",
    "To do this we can prepare the features as previously but without the imputation: we just replace missing values by the -1 marker value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:27.557084Z",
     "start_time": "2019-02-14T14:58:27.527819+01:00"
    }
   },
   "outputs": [],
   "source": [
    "features = pd.concat([data.get(['Fare', 'Age']),\n",
    "                      pd.get_dummies(data.Sex, prefix='Sex'),\n",
    "                      pd.get_dummies(data.Pclass, prefix='Pclass'),\n",
    "                      pd.get_dummies(data.Embarked, prefix='Embarked')],\n",
    "                     axis=1)\n",
    "features = features.drop('Sex_male', 1)\n",
    "\n",
    "# Because of the following bug we cannot use NaN as the missing\n",
    "# value marker, use a negative value as marker instead:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/3044\n",
    "features = features.fillna(-1)\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `Imputer` transformer of scikit-learn to find the median value on the training set and apply it on missing values of both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:28.750737Z",
     "start_time": "2019-02-14T14:58:28.744858+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features.values, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:29.498022Z",
     "start_time": "2019-02-14T14:58:29.488164+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median age computed on the training set is stored in the `statistics_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:30.769663Z",
     "start_time": "2019-02-14T14:58:30.763737+01:00"
    }
   },
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation can now happen by calling  the transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:31.780278Z",
     "start_time": "2019-02-14T14:58:31.776277+01:00"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:32.265425Z",
     "start_time": "2019-02-14T14:58:32.260355+01:00"
    }
   },
   "outputs": [],
   "source": [
    "np.any(X_train == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:32.787908Z",
     "start_time": "2019-02-14T14:58:32.783339+01:00"
    }
   },
   "outputs": [],
   "source": [
    "np.any(X_train_imputed == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:33.242134Z",
     "start_time": "2019-02-14T14:58:33.234270+01:00"
    }
   },
   "outputs": [],
   "source": [
    "np.any(X_test == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:33.727622Z",
     "start_time": "2019-02-14T14:58:33.716806+01:00"
    }
   },
   "outputs": [],
   "source": [
    "np.any(X_test_imputed == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a pipeline that wraps an imputer transformer and the classifier itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:35.183949Z",
     "start_time": "2019-02-14T14:58:34.774775+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
    "                                        subsample=.8, max_features=.5)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imp', imputer),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipeline, features.values, target, cv=5, n_jobs=4,\n",
    "                         scoring='accuracy', )\n",
    "print(scores.min(), scores.mean(), scores.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean cross-validation is slightly lower than we used the imputation on the whole data as we did earlier although not by much. This means that in this case the data-snooping was not really helping the model cheat by much.\n",
    "\n",
    "Let us re-run the grid search, this time on the pipeline. Note that thanks to the pipeline structure we can optimize the interaction of the imputation method with the parameters of the downstream classifier without cheating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T13:58:39.705628Z",
     "start_time": "2019-02-14T14:58:36.315715+01:00"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'imp__strategy': ['mean', 'median'],\n",
    "    'clf__max_features': [0.5, 1],\n",
    "    'clf__max_depth': [3, 4, 5],\n",
    "}\n",
    "gs = GridSearchCV(pipeline, params, cv=5, scoring='roc_auc', n_jobs=4)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:02:10.578651Z",
     "start_time": "2019-02-14T15:02:10.570381+01:00"
    }
   },
   "outputs": [],
   "source": [
    "order=np.argsort(gs.cv_results_['mean_test_score'])[::-1]\n",
    "for i in range(gs.cv_results_['mean_test_score'].shape[0]):\n",
    "    print('mean: %f, std: %f, params: %s' % \n",
    "          (gs.cv_results_['mean_test_score'][order[i]],\n",
    "           gs.cv_results_['std_test_score'][order[i]],\n",
    "           gs.cv_results_['params'][order[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:02:19.636863Z",
     "start_time": "2019-02-14T15:02:19.631428+01:00"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:03:06.613721Z",
     "start_time": "2019-02-14T15:03:06.385574+01:00"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, gs.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:03:09.029317Z",
     "start_time": "2019-02-14T15:03:09.022279+01:00"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example from Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll take a look at a simple facial recognition example.\n",
    "Ideally, we would use a dataset consisting of a\n",
    "subset of the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
    "data that is available within scikit-learn with the 'datasets.fetch_lfw_people' function. However, this is a relatively large download (~200MB) so we will do the tutorial on a simpler, less rich dataset. Feel free to explore the LFW dataset at home."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# faces = datasets.fetch_olivetti_faces()\n",
    "# faces.data.shape\n",
    "\n",
    "# np.save('olivetti_data.npy',faces['data'])\n",
    "# np.save('olivetti_images.npy',faces['images'])\n",
    "# np.save('olivetti_target.npy',faces['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:42:09.196283Z",
     "start_time": "2019-02-14T15:42:09.178832+01:00"
    }
   },
   "outputs": [],
   "source": [
    "faces={}\n",
    "faces['data']= np.load('./data/olivetti_data.npy')\n",
    "faces['images']= np.load('./data/olivetti_images.npy')\n",
    "faces['target']= np.load('./data/olivetti_target.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these faces to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:43:28.525676Z",
     "start_time": "2019-02-14T15:43:27.926373+01:00"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "# plot several images\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "#     ax.imshow(faces.images[i], cmap=plt.cm.bone)\n",
    "    ax.imshow(faces['images'][i], cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that these faces have already been localized and scaled\n",
    "to a common size.  This is an important preprocessing piece for facial\n",
    "recognition, and is a process that can require a large collection of training\n",
    "data.  This can be done in scikit-learn, but the challenge is gathering a\n",
    "sufficient amount of training data for the algorithm to work\n",
    "\n",
    "Fortunately, this piece is common enough that it has been done.  One good\n",
    "resource is [OpenCV](https://docs.opencv.org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html), the\n",
    "*Open Computer Vision Library*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform a Support Vector classification of the images.  We'll\n",
    "do a typical train-test split on the images to make this happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:45:58.807969Z",
     "start_time": "2019-02-14T15:45:58.795228+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(faces.data,\n",
    "#         faces.target, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(faces['data'],\n",
    "        faces['target'], random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4096 dimensions is a lot for SVM.  We can use PCA to reduce these 4096 features to a manageable\n",
    "size, while maintaining most of the information in the dataset.  Here it is useful to use a variant\n",
    "of PCA called ``RandomizedPCA``, which is an approximation of PCA that can be much faster for large\n",
    "datasets.  The interface is the same as the normal PCA we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:46:51.631046Z",
     "start_time": "2019-02-14T15:46:51.334396+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(svd_solver='randomized',n_components=150, whiten=True)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting part of PCA is that it computes the \"mean\" face, which can be\n",
    "interesting to examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:47:18.424444Z",
     "start_time": "2019-02-14T15:47:18.235628+01:00"
    }
   },
   "outputs": [],
   "source": [
    "# plt.imshow(pca.mean_.reshape(faces.images[0].shape),\n",
    "#            cmap=plt.cm.bone)\n",
    "\n",
    "plt.imshow(pca.mean_.reshape(faces['images'][0].shape),\n",
    "           cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components measure deviations about this mean along orthogonal axes.\n",
    "It is also interesting to visualize these principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:47:37.073103Z",
     "start_time": "2019-02-14T15:47:37.068791+01:00"
    }
   },
   "outputs": [],
   "source": [
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:48:08.503682Z",
     "start_time": "2019-02-14T15:48:07.398530+01:00"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "#     ax.imshow(pca.components_[i].reshape(faces.images[0].shape), cmap=plt.cm.bone)\n",
    "    ax.imshow(pca.components_[i].reshape(faces['images'][0].shape), cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components (\"eigenfaces\") are ordered by their importance from top-left to bottom-right.\n",
    "We see that the first few components seem to primarily take care of lighting\n",
    "conditions; the remaining components pull out certain identifying features:\n",
    "the nose, eyes, eyebrows, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this projection computed, we can now project our original training\n",
    "and test data onto the PCA basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:48:54.882761Z",
     "start_time": "2019-02-14T15:48:54.856951+01:00"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:49:00.368216Z",
     "start_time": "2019-02-14T15:49:00.363426+01:00"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These projected components correspond to factors in a linear combination of\n",
    "component images such that the combination approaches the original face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the Learning: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform support-vector-machine classification on this reduced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:49:45.600604Z",
     "start_time": "2019-02-14T15:49:45.538012+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(C=5., gamma=0.001)\n",
    "clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate how well this classification did.  First, we might plot a\n",
    "few of the test-cases with the labels learned from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca[0][np.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:50:31.244705Z",
     "start_time": "2019-02-14T15:50:30.640362+01:00"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "for i in range(15):\n",
    "    ax = fig.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "#     ax.imshow(X_test[i].reshape(faces.images[0].shape),\n",
    "#               cmap=plt.cm.bone)\n",
    "    ax.imshow(X_test[i].reshape(faces['images'][0].shape),\n",
    "              cmap=plt.cm.bone)\n",
    "    y_pred = clf.predict(X_test_pca[i][np.newaxis,:])[0]\n",
    "    color = ('black' if y_pred == y_test[i]\n",
    "             else 'red')\n",
    "    ax.set_title(y_pred, fontsize='small', color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is correct on an impressive number of images given the simplicity\n",
    "of its learning model!  Using a linear classifier on 150 features derived from\n",
    "the pixel-level data, the algorithm correctly identifies a large number of the\n",
    "people in the images.\n",
    "\n",
    "Again, we can\n",
    "quantify this effectiveness using one of several measures from the ``sklearn.metrics``\n",
    "module.  First we can do the classification report, which shows the precision,\n",
    "recall and other measures of the \"goodness\" of the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:50:50.099349Z",
     "start_time": "2019-02-14T15:50:50.086461+01:00"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting metric is the *confusion matrix*, which indicates how often\n",
    "any two items are mixed-up.  The confusion matrix of a perfect classifier\n",
    "would only have nonzero entries on the diagonal, with zeros on the off-diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T14:51:11.155094Z",
     "start_time": "2019-02-14T15:51:11.148177+01:00"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Note\n",
    "\n",
    "Here we have used PCA \"eigenfaces\" as a pre-processing step for facial recognition.\n",
    "The reason we chose this is because PCA is a broadly-applicable technique, which can\n",
    "be useful for a wide array of data types.  Research in the field of facial recognition\n",
    "in particular, however, has shown that other more specific feature extraction methods\n",
    "are can be much more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to:\n",
    "\n",
    "- Again [Bartosz Tele≈Ñczuk](https://datascience.telenczuk.pl/teaching/) and [Software Carpentry team](https://software-carpentry.org/) for providing notebook from which these are inspired\n",
    "\n",
    "- Kaggle for setting up the Titanic challenge.\n",
    "\n",
    "- This blog post by Philippe Adjiman for inspiration:\n",
    "\n",
    "http://www.philippeadjiman.com/blog/2013/09/12/a-data-science-exploration-from-the-titanic-in-r/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
