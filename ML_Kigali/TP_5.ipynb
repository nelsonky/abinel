{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:34.195707Z",
     "start_time": "2019-02-14T11:08:33.534286Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic principles of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we start diving into the field of machine learning.\n",
    "\n",
    "By the end of this section you will\n",
    "\n",
    "- Know the basic categories of supervised learning, including classification and regression problems.\n",
    "- Know the basic categories of unsupervised learning, including dimensionality reduction and clustering.\n",
    "- Know the basic syntax of the Scikit-learn **estimator** interface.\n",
    "- Know how features are extracted from real-world data.\n",
    "\n",
    "In addition, we will go over several basic tools within scikit-learn which can be used to accomplish the above tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple definition of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (ML) is about building programs with **tunable parameters** (typically an\n",
    "array of floating point values) that are adjusted automatically so as to improve\n",
    "their behavior by **adapting to previously seen data.**\n",
    "\n",
    "In most ML applications, the data is in a 2D array of shape ``[n_samples x n_features]``,\n",
    "where the number of features is the same for each object, and each feature column refers\n",
    "to a related piece of information about each sample.\n",
    "\n",
    "Nowadays Machine learning can be broken into three broad regimes:\n",
    "*supervised learning*, *unsupervised learning* and *reinforcement learning*.\n",
    "Weâ€™ll introduce the main concepts of the first two types here, and discuss them in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the scikit-learn estimator object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. For instance a linear regression is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:34.990490Z",
     "start_time": "2019-02-14T11:08:34.198440Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# what starts with a capital letter is typically a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:34.998162Z",
     "start_time": "2019-02-14T11:08:34.992455Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(normalize=True)\n",
    "print(model.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.007878Z",
     "start_time": "2019-02-14T11:08:35.000636Z"
    }
   },
   "outputs": [],
   "source": [
    "LinearRegression?\n",
    "# to look at the source code LinearRegression?? (two question marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.017263Z",
     "start_time": "2019-02-14T11:08:35.010339Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "# it shows what parameter I can set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.036625Z",
     "start_time": "2019-02-14T11:08:35.019355Z"
    }
   },
   "outputs": [],
   "source": [
    "# another way\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimated parameters**: When data is fitted with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.056339Z",
     "start_time": "2019-02-14T11:08:35.048957Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 2])\n",
    "y = np.array([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.444645Z",
     "start_time": "2019-02-14T11:08:35.060915Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(x, y, marker='o')\n",
    "# without \"_ =\" I will have also a line saying several stuffs. So I return it without giving a name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.483846Z",
     "start_time": "2019-02-14T11:08:35.462446Z"
    }
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.509531Z",
     "start_time": "2019-02-14T11:08:35.492346Z"
    }
   },
   "outputs": [],
   "source": [
    "X = x[:, np.newaxis] # The input data for sklearn is 2D: (samples == 3 x features == 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.541009Z",
     "start_time": "2019-02-14T11:08:35.518055Z"
    }
   },
   "outputs": [],
   "source": [
    "_, n_features = X.shape\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.615558Z",
     "start_time": "2019-02-14T11:08:35.548664Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, y) \n",
    "model.coef_\n",
    "# the _ is not accident: what is estimated from data. No _ is input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.624814Z",
     "start_time": "2019-02-14T11:08:35.618790Z"
    }
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Supervised Learning**, we have a dataset consisting of both features and labels.\n",
    "The task is to construct an estimator which is able to predict the label of an object\n",
    "given the set of features. A relatively simple example is predicting the species of \n",
    "iris given a set of measurements of its flower. This is a relatively simple task. \n",
    "Some more complicated examples are:\n",
    "\n",
    "- given a multicolor image of an object through a telescope, determine\n",
    "  whether that object is a star, a quasar, or a galaxy.\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like\n",
    "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
    "\n",
    "What these tasks have in common is that there is one or more unknown\n",
    "quantities associated with the object which needs to be determined from other\n",
    "observed quantities.\n",
    "\n",
    "Supervised learning is further broken down into two categories, **classification** and **regression**.\n",
    "In classification, the label is discrete, while in regression, the label is continuous. For example,\n",
    "in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a\n",
    "classification problem: the label is from three distinct categories. On the other hand, we might\n",
    "wish to estimate the age of an object based on such observations: this would be a regression problem,\n",
    "because the label (age) is a continuous quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors (kNN) is one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
    "\n",
    "Let's try it out on our iris classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:35.764433Z",
     "start_time": "2019-02-14T11:08:35.627191Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "print(iris.target_names[knn.predict([[3, 5, 4, 2]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A plot of the sepal space and the prediction of the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:36.158340Z",
     "start_time": "2019-02-14T11:08:35.777134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create color maps for 3-class classification problem, as with iris\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "def plot_iris_knn():\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                        # avoid this ugly slicing by using a two-dim dataset\n",
    "    y = iris.target\n",
    "\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.figure()\n",
    "    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "    pl.xlabel('sepal length (cm)')\n",
    "    pl.ylabel('sepal width (cm)')\n",
    "    pl.axis('tight')\n",
    "    \n",
    "plot_iris_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now use other estimators on the same problem. For ex:) sklearn.svm.SVC, \n",
    "\n",
    "> Note that you don't have to know what it is to use it. Try to answer the question: What kind of iris has 5cm x 6cm sepal and 1cm x 2cm petal?\n",
    "\n",
    "> If you finish early, do the same plot as above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest possible regression setting is the linear regression one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:36.530452Z",
     "start_time": "2019-02-14T11:08:36.162562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create some simple data\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.random(size=(20, 1))\n",
    "y = 3 * X[:, 0] + 2 + np.random.normal(size=20)\n",
    "\n",
    "# Fit a linear regression to it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "print(\"Model coefficient: %.5f, and intercept: %.5f\"\n",
    "      % (model.coef_, model.intercept_))\n",
    "\n",
    "# Plot the data and the model prediction\n",
    "X_test = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "y_test = model.predict(X_test)\n",
    "plt.plot(X[:, 0], y, 'o')\n",
    "plt.plot(X_test[:, 0], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A recap on Scikit-learn's estimator interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn strives to have a uniform interface across all methods,\n",
    "and weâ€™ll see examples of these below. Given a scikit-learn *estimator*\n",
    "object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "  + `model.fit()` : fit training data. For supervised learning applications,\n",
    "    this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`).\n",
    "    For unsupervised learning applications, this accepts only a single argument,\n",
    "    the data `X` (e.g. `model.fit(X)`).\n",
    "- Available in **supervised estimators**\n",
    "  + `model.predict()` : given a trained model, predict the label of a new set of data.\n",
    "    This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`),\n",
    "    and returns the learned label for each object in the array.\n",
    "  + `model.predict_proba()` : For classification problems, some estimators also provide\n",
    "    this method, which returns the probability that a new observation has each categorical label.\n",
    "    In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "  + `model.score()` : for classification or regression problems, most (all?) estimators implement\n",
    "    a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "- Available in **unsupervised estimators**\n",
    "  + `model.transform()` : given an unsupervised model, transform new data into the new basis.\n",
    "    This also accepts one argument `X_new`, and returns the new representation of the data based\n",
    "    on the unsupervised model.\n",
    "  + `model.fit_transform()` : some estimators implement this method,\n",
    "    which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: what it is and why it is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train errors** Suppose you are using a 1-nearest neighbor estimator. How many errors do you expect on your train set?\n",
    "\n",
    "This tells us that:\n",
    "- Train set error is not a good measurement of prediction performance. You need to leave out a test set.\n",
    "- In general, we should accept errors on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example of regularization** The core idea behind regularization is that we are going to prefer models that are simpler, for a certain definition of ''simpler'', even if they lead to more errors on the train set.\n",
    "\n",
    "As an example, let's generate with a 9th order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:37.045926Z",
     "start_time": "2019-02-14T11:08:36.546572Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "x = 2 * rng.rand(100) - 1\n",
    "\n",
    "f = lambda t: 1.2 * t ** 2 + .1 * t ** 3 - .4 * t ** 5 - .5 * t ** 9\n",
    "y = f(x) + .4 * rng.normal(size=100)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's fit a 4th order and a 9th order polynomial to the data. For this we need to engineer features: the n_th powers of x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:37.535615Z",
     "start_time": "2019-02-14T11:08:37.050768Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.linspace(-1, 1, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=4)\n",
    "\n",
    "X = np.array([x**i for i in range(5)]).T\n",
    "X_test = np.array([x_test**i for i in range(5)]).T\n",
    "order4 = LinearRegression()\n",
    "order4.fit(X, y)\n",
    "plt.plot(x_test, order4.predict(X_test), label='4th order')\n",
    "\n",
    "X = np.array([x**i for i in range(10)]).T\n",
    "X_test = np.array([x_test**i for i in range(10)]).T\n",
    "order9 = LinearRegression()\n",
    "order9.fit(X, y)\n",
    "plt.plot(x_test, order9.predict(X_test), label='9th order')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.axis('tight')\n",
    "plt.title('Fitting a 4th and a 9th order polynomial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your naked eyes, which model do you prefer, the 4th order one, or the 9th order one?\n",
    "\n",
    "Let's look at the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:37.900458Z",
     "start_time": "2019-02-14T11:08:37.541395Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x, y, s=4)\n",
    "plt.plot(x_test, f(x_test), label=\"truth\")\n",
    "plt.axis('tight')\n",
    "plt.title('Ground truth (9th order polynomial)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is ubiquitous in machine learning. Most scikit-learn estimators have a parameter to tune the amount of regularization. For instance, with k-NN, it is 'k', the number of nearest neighbors used to make the decision. k=1 amounts to no regularization: 0 error on the training set, whereas large k will push toward smoother decision boundaries in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "______\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is interested in situations in which X is available, but not y: data without labels.\n",
    "\n",
    "A typical use case is to find hiden structure in the data.\n",
    "\n",
    "Previously we worked on visualizing the iris data by plotting\n",
    "pairs of dimensions by trial and error, until we arrived at\n",
    "the best pair of dimensions for our dataset.  Here we will\n",
    "use an unsupervised *dimensionality reduction* algorithm\n",
    "to accomplish this more automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this section you will\n",
    "\n",
    "- Know how to instantiate and train an unsupervised dimensionality reduction algorithm:\n",
    "  Principal Component Analysis (PCA)\n",
    "- Know how to use PCA to visualize high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is the task of deriving a set of new\n",
    "artificial features that is smaller than the original feature\n",
    "set while retaining most of the variance of the original data.\n",
    "Here we'll use a common but powerful dimensionality reduction\n",
    "technique called Principal Component Analysis (PCA).\n",
    "We'll perform PCA on the iris dataset that we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:37.928498Z",
     "start_time": "2019-02-14T11:08:37.905427Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is performed using linear combinations of the original features\n",
    "using a truncated Singular Value Decomposition of the matrix X so\n",
    "as to project the data onto a base of the top singular vectors.\n",
    "If the number of retained components is 2 or 3, PCA can be used\n",
    "to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.001369Z",
     "start_time": "2019-02-14T11:08:37.931506Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fitted, the pca model exposes the singular vectors in the components_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.016665Z",
     "start_time": "2019-02-14T11:08:38.006119Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T18:42:39.721681Z",
     "start_time": "2019-02-13T18:42:39.716956Z"
    }
   },
   "source": [
    "Other attributes are available as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.033648Z",
     "start_time": "2019-02-14T11:08:38.026838Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.043073Z",
     "start_time": "2019-02-14T11:08:38.036282Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us project the iris dataset along those first two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.052981Z",
     "start_time": "2019-02-14T11:08:38.047910Z"
    }
   },
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA `normalizes` and `whitens` the data, which means that the data\n",
    "is now centered on both components with unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.070921Z",
     "start_time": "2019-02-14T11:08:38.054878Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.round(X_pca.mean(axis=0), decimals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.084031Z",
     "start_time": "2019-02-14T11:08:38.075741Z"
    }
   },
   "outputs": [],
   "source": [
    "np.round(X_pca.std(axis=0), decimals=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T18:44:01.038157Z",
     "start_time": "2019-02-13T18:44:01.033429Z"
    }
   },
   "source": [
    "Furthermore, the samples components do no longer carry any linear correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.093028Z",
     "start_time": "2019-02-14T11:08:38.086272Z"
    }
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X_pca.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the results using the following utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.116022Z",
     "start_time": "2019-02-14T11:08:38.103223Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "def plot_PCA_2D(data, target, target_names):\n",
    "    colors = cycle('rgbcmykw')\n",
    "    target_ids = range(len(target_names))\n",
    "    plt.figure()\n",
    "    for i, c, label in zip(target_ids, colors, target_names):\n",
    "        plt.scatter(data[target == i, 0], data[target == i, 1],\n",
    "                   c=c, label=label)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calling this function for our data, we see the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:38.478158Z",
     "start_time": "2019-02-14T11:08:38.117888Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_PCA_2D(X_pca, iris.target, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this projection was determined *without* any information about the\n",
    "labels (represented by the colors): this is the sense in which the learning\n",
    "is **unsupervised**.  Nevertheless, we see that the projection gives us insight\n",
    "into the distribution of the different flowers in parameter space: notably,\n",
    "*iris setosa* is much more distinct than the other two species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the default implementation of PCA computes the\n",
    "singular value decomposition (SVD) of the full\n",
    "data matrix, which is not scalable when both ``n_samples`` and\n",
    "``n_features`` are big (more that a few thousands).\n",
    "If you are interested in a number of components that is much\n",
    "smaller than both ``n_samples`` and ``n_features``, consider using\n",
    "`sklearn.decomposition.RandomizedPCA` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T18:52:16.649112Z",
     "start_time": "2019-02-13T18:52:16.639994Z"
    }
   },
   "source": [
    "Other dimensionality reduction techniques which are useful to know about:\n",
    "\n",
    "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html): \n",
    "   Principal Component Analysis\n",
    "- [sklearn.decomposition.RandomizedPCA](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.decomposition.RandomizedPCA.html):\n",
    "   fast non-exact PCA implementation based on a randomized algorithm\n",
    "- [sklearn.decomposition.SparsePCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html):\n",
    "   PCA variant including L1 penalty for sparsity\n",
    "- [sklearn.decomposition.FastICA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html):\n",
    "   Independent Component Analysis\n",
    "- [sklearn.decomposition.NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html):\n",
    "   non-negative matrix factorization\n",
    "- [sklearn.manifold.LocallyLinearEmbedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html):\n",
    "   nonlinear manifold learning technique based on local neighborhood geometry\n",
    "- [sklearn.manifold.IsoMap](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html):\n",
    "   nonlinear manifold learning technique based on a sparse graph algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One weakness of PCA is that it cannot detect non-linear features.  A set\n",
    "of algorithms known as *Manifold Learning* have been developed to address\n",
    "this deficiency.  A canonical dataset used in Manifold learning is the\n",
    "*S-curve*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:44:29.152366Z",
     "start_time": "2019-02-14T11:44:28.824731Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "X, y = make_s_curve(n_samples=1000)\n",
    "# change the proportions to emphasize the weakness of PCA\n",
    "X[:, 1] -= 1\n",
    "X[:, 1] *= 1.5\n",
    "X[:, 2] *= 0.5\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(X[:, 0], X[:, 1], X[:, 2], c=y)\n",
    "ax.view_init(10, -60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 2-dimensional dataset embedded in three dimensions, but it is embedded\n",
    "in such a way that PCA cannot discover the underlying data orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:44:31.991832Z",
     "start_time": "2019-02-14T11:44:31.676209Z"
    }
   },
   "outputs": [],
   "source": [
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning algorithms, however, available in the ``sklearn.manifold``\n",
    "submodule, are able to recover the underlying 2-dimensional manifold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:40.269583Z",
     "start_time": "2019-02-14T11:08:39.286051Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding, Isomap\n",
    "lle = LocallyLinearEmbedding(n_neighbors=15, n_components=2, method='modified')\n",
    "X_lle = lle.fit_transform(X)\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:41.298332Z",
     "start_time": "2019-02-14T11:08:40.271511Z"
    }
   },
   "outputs": [],
   "source": [
    "iso = Isomap(n_neighbors=15, n_components=2)\n",
    "X_iso = iso.fit_transform(X)\n",
    "plt.scatter(X_iso[:, 0], X_iso[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Dimension reduction of digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T18:55:25.549785Z",
     "start_time": "2019-02-13T18:55:25.544526Z"
    }
   },
   "source": [
    "Apply PCA, LocallyLinearEmbedding, and Isomap to project the data to two dimensions.\n",
    "Which visualization technique separates the classes most cleanly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:41.480997Z",
     "start_time": "2019-02-14T11:08:41.303412Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T11:08:41.492627Z",
     "start_time": "2019-02-14T11:08:41.485492Z"
    }
   },
   "outputs": [],
   "source": [
    "%load solutions/08A_digits_projection.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
